<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hanhui Wang</title>
    <meta name="author" content="Hanhui  Wang" />
    <meta name="description" content="Don't waste your life. Don't waste it.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>☯️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sarihust.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class=" sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="mailto:%77%61%6E%67.%68%61%6E%68@%6E%6F%72%74%68%65%61%73%74%65%72%6E.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=ML8KeAEAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/sarihust" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/hanhuiwang" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/hhwang1108" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="./assets/img/wechat.jpg" title="WeChat"><i class="fa fa-wechat"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!-- Blogs -->
              <!-- <li class="nav-item">
                <a class="nav-link" href="https://normaluhr.github.io" target="_blank" rel="noopener noreferrer">Blogs</a>
              </li> -->

              <!-- CV -->
              <li class="nav-item">
                <a class="nav-link" href="/assets/pdf/cv.pdf" target="_blank" rel="noopener noreferrer">CV</a>
              </li>
              
              <!-- Blog -->
              <!-- Uncomment below to use in-site blogs -->
              <!--  -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Hanhui Wang
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hhwang_original-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hhwang_original-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hhwang_original-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hhwang_original.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="hhwang_original.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              <p>West Village Residence</p> <p>440 Hunting Avenue</p> <p>Boston, Massachusetts</p> <p>United States of America</p> <p>             </p> <p>             </p>

            </div>
          </div>

          <div class="clearfix">
            <p>Hanhui Wang (王翰辉) is an incoming Ph.D. student at the Visual Intelligence Group at <strong>Northeastern University</strong> (NEU), where he will be advised by <a href="https://jianghz.me/" target="_blank" rel="noopener noreferrer">Prof. Huaizu Jiang</a>. His previous research has centered on <em>3D Computer Vision</em>, <em>Multimodal Learning</em>, and <em>Generative AI</em>, with a particular focus on spatial reasoning, 3D scene understanding, and the safety of generative models. Prior to joining NEU, he received his bachelor’s degree from <strong>Huazhong University of Science and Technology</strong> (HUST) and his master’s degree from the <strong>University of Southern California</strong> (USC). He has had the honor of collaborating with <a href="https://nini-lxz.github.io/" target="_blank" rel="noopener noreferrer">Prof. Xianzhi Li</a>, <a href="https://vztu.github.io/" target="_blank" rel="noopener noreferrer">Prof. Zhengzhong Tu</a>, and <a href="https://jianghz.me/" target="_blank" rel="noopener noreferrer">Prof. Huaizu Jiang</a> on a range of research topics spanning diverse areas of computer vision. He also gained valuable industry experience through internships at leading technology companies such as <a href="https://www.iflytek.com/en/" target="_blank" rel="noopener noreferrer">iFLYTEK</a>.</p>

<p><strong>Research Keywords</strong>: Machine Unlearning, Aritificial Intelligence, Computer Vision, 3D Vision, Multimodality, Generative AI.</p>

<!-- :heavy_check_mark: **Theme 1: Trustworthy Foundation Models: Robustness, Fairness, and Unlearning**: Yihua explores how to enhance the trustworthiness of foundation models, focusing on robustness against adversarial attacks, fairness in decision-making, and the emerging area of machine unlearning to ensure data privacy and compliance with deletion requests.

:heavy_check_mark: **Theme 2: Scalable Foundation Models: Efficient Models, Data, and Algorithms**: In this theme, Yihua's work revolves around designing models that are not only powerful but also computationally efficient. His research includes advancements in model sparsification, memory-efficient fine-tuning techniques, and optimizing data usage for large-scale models.

:heavy_check_mark: **Theme 3: Optimization in Modern ML: Bi-Level and Zeroth-Order Optimization**
This research line focuses on the theoretical underpinnings of scalable machine learning algorithms, addressing real-world constraints through bi-level optimization and zeroth-order optimization.

**Collaboration Opportunities**

I am always open to collaborations with researchers, as well as undergraduate and graduate students seeking Ph.D. positions. While my primary research focuses on trustworthy and scalable ML algorithms for LLMs and DMs, I am also interested in exploring a wide range of topics beyond these areas. If you have exciting research ideas or are looking for opportunities to conduct research under professional guidance, feel free to reach out to me. Please refer to my [collaboration statement](./collaboration) for more details. You are also welcome to befriend me on [Wechat](./assets/img/Wechat.jpg) or connect me through [LinkedIn](https://www.linkedin.com/in/zhangyihua/). -->


          </div>

          <!-- News -->          
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive" style="max-height: 40vw">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row">Jun 7, 2025</th>
                  <td>
                    <img class="emoji" title=":hammer_and_wrench:" alt=":hammer_and_wrench:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png" height="20" width="20"> After putting it off for way too long, I finally rebuilt my personal website. It’s cleaner, more up to date, and a little closer to how I want to present my work (for now).
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jun 2, 2025</th>
                  <td>
                    <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2506.04220" target="_blank" rel="noopener noreferrer">Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models</a> was selected as a <b>spotlight</b> presentation at the <b>CVPR 2025 Workshop on Computer Vision in the Wild (CVinW)</b>! I will be presenting it in <b>Nashville</b> on <b>June 11th</b>!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 26, 2025</th>
                  <td>
                    <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2411.16832" target="_blank" rel="noopener noreferrer">Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing</a> is accepted to CVPR 2025!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 6, 2025</th>
                  <td>
                    <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> I’m excited to share that I have accepted a Ph.D. offer at Northeastern University, where I will be joining the Visual Intelligence Group under the supervision of <a href="https://jianghz.me/" target="_blank" rel="noopener noreferrer">Prof. Huaizu Jiang</a>. Looking forward to this new chapter in Boston!
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>First-Authored Publications</h2>
            
            <h5 class="year">See a full publication list at <a href="publications/">here</a>.</h5>
            <br>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
  <div class="row">

    <div class="col-sm-3 abbr">
      
            <abbr class="badge">CVinW’25</abbr>
        
      
      
        <img src="/assets/img/publication_preview/struct2d.png" class="teaser img-fluid z-depth-1">
      
    </div>

        <!-- Entry bib key -->
        <div id="zhu2025struct2dperceptionguidedframeworkspatial" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models</div>
          <!-- Author -->
          <div class="author">
          

          Fangrui Zhu*, <em>Hanhui Wang*</em>, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, and Huaizu Jiang</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In The 4th CVPR Workshop on Computer Vision in the Wild</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!--
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> -->
            <a href="https://arxiv.org/abs/2506.04220" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>
    Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can LMMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines bird’s-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source LMM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in LMMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research.
  </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2025struct2dperceptionguidedframeworkspatial</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu*, Fangrui and Wang*, Hanhui and Xie, Yiming and Gu, Jing and Ding, Tianye and Yang, Jianwei and Jiang, Huaizu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 4th CVPR Workshop on Computer Vision in the Wild}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
  <div class="row">

    <div class="col-sm-3 abbr">
      
            <abbr class="badge">CVPR’25</abbr>
        
      
      
        <img src="/assets/img/publication_preview/facelock.png" class="teaser img-fluid z-depth-1">
      
    </div>

        <!-- Entry bib key -->
        <div id="wang2025edit" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing</div>
          <!-- Author -->
          <div class="author">
          

          <em>Hanhui Wang*</em>, Yihua Zhang*, Ruizheng Bai, Yue Zhao, Sijia Liu, and Zhengzhong Tu</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In The IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!--
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> -->
            <a href="https://arxiv.org/pdf/2411.16832" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/taco-group/FaceLock" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="/assets/pdf/posters/CVPR25_FaceLock_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent advancements in diffusion models have made generative image editing more accessible than ever. While these developments allow users to generate creative edits with ease, they also raise significant ethical concerns, particularly regarding malicious edits to human portraits that threaten individuals’ privacy and identity security. Existing general-purpose image protection methods primarily focus on generating adversarial perturbations to nullify edit effects. However, these approaches often exhibit instability to protect against diverse editing requests. In this work, we introduce a novel perspective to personal human portrait protection against malicious editing. Unlike traditional methods aiming to prevent edits from taking effect, our method, FACELOCK, optimizes adversarial perturbations to ensure that original biometric information—such as facial features—is either destroyed or substantially altered post-editing, rendering the subject in the edited output biometrically unrecognizable. Our approach innovatively integrates facial recognition and visual perception factors into the perturbation optimization process, ensuring robust protection against a variety of editing attempts. Besides, we shed light on several critical issues with commonly used evaluation metrics in image editing and reveal cheating methods by which they can be easily manipulated, leading to deceptive assessments of protection. Through extensive experiments, we demonstrate that FACELOCK significantly outperforms all baselines in defense performance against a wide range of malicious edits. Moreover, our method also exhibits strong robustness against purification techniques. Comprehensive ablation studies confirm the stability and broad applicability of our method across diverse diffusion-based editing algorithms. Our work not only advances the state-of-the-art in biometric defense but also sets the foundation for more secure and privacy-preserving practices in image editing. The code is publicly available at: https://github.com/taco-group/FaceLock.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2025edit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang*, Hanhui and Zhang*, Yihua and Bai, Ruizheng and Zhao, Yue and Liu, Sijia and Tu, Zhengzhong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
  <div class="row">

    <div class="col-sm-3 abbr">
      
            <abbr class="badge">arXiv’2024</abbr>
        
      
      
        <img src="/assets/img/publication_preview/leveraging_SAM.png" class="teaser img-fluid z-depth-1">
      
    </div>

        <!-- Entry bib key -->
        <div id="wang2024leveragingsamsinglesourcedomain" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Hanhui Wang</em>, Ye Huaize, Xia Yi, and Zhang Xueyan</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In arXiv</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!--
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> -->
            <a href="https://arxiv.org/pdf/2401.02076" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/sarihust/SAMMed" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Domain Generalization (DG) aims to reduce domain shifts between domains to achieve promising performance on the unseen target domain, which has been widely practiced in medical image segmentation. Single-source domain generalization (SDG) is the most challenging setting that trains on only one source domain. Although existing methods have made considerable progress on SDG of medical image segmentation, the performances are still far from the applicable standards when faced with a relatively large domain shift. In this paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve the ability of generalization. Specifically, we introduce a parallel framework, the source images are sent into the SAM module and normal segmentation module respectively. To reduce the calculation resources, we apply a merging strategy before sending images to the SAM module. We extract the bounding boxes from the segmentation module and send the refined version as prompts to the SAM module. We evaluate our model on a classic DG dataset and achieve competitive results compared to other state-of-the-art DG methods. Furthermore, We conducted a series of ablation experiments to prove the effectiveness of the proposed method. The code is publicly available at: https://github.com/sarihust/SAMMed.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2024leveragingsamsinglesourcedomain</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Hanhui and Huaize, Ye and Yi, Xia and Xueyan, Zhang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%77%61%6E%67.%68%61%6E%68@%6E%6F%72%74%68%65%61%73%74%65%72%6E.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=ML8KeAEAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/sarihust" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/hanhuiwang" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/hhwang1108" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="./assets/img/wechat.jpg" title="WeChat"><i class="fa fa-wechat"></i></a>
            

            </div>

            <div class="contact-note">
              
            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <center>
        <div id="clustrmaps-widget" style="width:10%">
                <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=sPS9KNUORGU9lFD7pat0DDuO2gpjdQjbOYGcN6-vNC0"></script>
        </div>
      <div class="container">
        © Copyright 2025 Hanhui  Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>. Inspired by <a href="https://yihua-zhang.com" target="_blank" rel="noopener noreferrer">Yihua Zhang</a>'s website.
Last updated: June 07, 2025.
      </div>
      </center>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
  $(function () {$('[data-toggle="tooltip"]').tooltip()})
  </script>
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };

    window.MathJax = {
    tex: {
      tags: 'ams'
      inlineMath: [['$', '$'], ['\\(', '\\)'], ['!!', '!!']]
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>
  <!-- <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$']]
        packages: ['base', 'newcommand', 'configMacros']
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script> -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
